{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7a96cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "from utils import clean_sentence\n",
    "from data_loader import get_loader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define a transform to pre-process the testing images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(224),                      # get 224x224 crop from the center\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Create the data loader\n",
    "data_loader = get_loader(transform=transform_test,\n",
    "                         mode='test')\n",
    "\n",
    "def get_caption(image_path):\n",
    "    # Define the transformation to preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    # Load the image and apply the transformation\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "\n",
    "    # Add a batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the most recent checkpoint\n",
    "    checkpoint = torch.load(os.path.join('./models', 'best-model.pkl'))\n",
    "\n",
    "    # Specify values for embed_size and hidden_size\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "\n",
    "    # Get the vocabulary and its size\n",
    "    vocab = data_loader.dataset.vocab\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Initialize the encoder and decoder, and set each to inference mode\n",
    "    encoder = EncoderCNN(embed_size).to(device)\n",
    "    encoder.eval()\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load the pre-trained weights\n",
    "    encoder.load_state_dict(checkpoint['encoder'])\n",
    "    decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "    # Move the models to the same device as the image\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    # Obtain the embedded image features\n",
    "    features = encoder(image)\n",
    "\n",
    "    # Reshape the features tensor to have the expected shape\n",
    "    features = features.unsqueeze(1)\n",
    "\n",
    "    # Pass the embedded image features through the model to get a predicted caption\n",
    "    output = decoder.sample(features)\n",
    "    \n",
    "    # Clean up the predicted caption\n",
    "    sentence = clean_sentence(output, vocab)\n",
    "\n",
    "    # Return the caption\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa3bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caption1(image_path):\n",
    "    # Define the transformation to preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    # Load the image and apply the transformation\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "\n",
    "    # Add a batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the most recent checkpoint\n",
    "    checkpoint = torch.load(os.path.join('./models', 'best-model.pkl'))\n",
    "\n",
    "    # Specify values for embed_size and hidden_size\n",
    "    embed_size = 256\n",
    "    hidden_size = 512\n",
    "\n",
    "    # Get the vocabulary and its size\n",
    "    vocab = data_loader.dataset.vocab\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Initialize the encoder and decoder, and set each to inference mode\n",
    "    encoder = EncoderCNN(embed_size).to(device)\n",
    "    encoder.eval()\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "    decoder.eval()\n",
    "\n",
    "    # Load the pre-trained weights\n",
    "    encoder.load_state_dict(checkpoint['encoder'])\n",
    "    decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "    # Move the models to the same device as the image\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    # Obtain the embedded image features\n",
    "    features = encoder(image)\n",
    "\n",
    "    # Reshape the features tensor to have the expected shape\n",
    "    features = features.unsqueeze(1)\n",
    "\n",
    "    # Pass the embedded image features through the model to get a predicted caption\n",
    "    output1 = decoder.sample_beam_search(features)\n",
    "    \n",
    "    # Clean up the predicted caption\n",
    "    sentence = clean_sentence(output1, vocab)\n",
    "\n",
    "    # Return the caption\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51574150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a dog laying on a blanket on a bed .\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load and display the image\n",
    "image_path = \"C:\\\\Users\\\\91887\\\\OneDrive - bmsce.ac.in\\\\Pictures\\\\Camera Roll\\\\IMG-20200711-WA0005.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image.show()\n",
    "# Get the caption for the image\n",
    "caption = get_caption(image_path)\n",
    "\n",
    "print(\"Caption:\", caption)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title('transformed image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3038d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load and display the image\n",
    "image_path = \"C:\\\\Users\\\\91887\\\\OneDrive - bmsce.ac.in\\\\Pictures\\\\Camera Roll\\\\IMG-20200711-WA0005.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image.show()\n",
    "# Get the caption for the image\n",
    "caption = get_caption1(image_path)\n",
    "\n",
    "print(\"Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd671cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\image_captioning\\\\cocoapi\\\\images\\\\test2014\\\\COCO_test2014_000000000027.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\Users\\\\91887\\\\OneDrive - bmsce.ac.in\\\\Pictures\\\\hum paanch\\\\IMG-20200916-WA0000.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2882f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\Users\\\\91887\\\\OneDrive - bmsce.ac.in\\\\Pictures\\\\Camera Roll\\\\IMG-20200711-WA0005.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\image_captioning\\\\cocoapi\\\\images\\\\test2014\\\\COCO_test2014_000000001110.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\image_captioning\\\\cocoapi\\\\images\\\\test2014\\\\COCO_test2014_000000001671.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ad10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\Users\\\\91887\\\\OneDrive - bmsce.ac.in\\\\Pictures\\\\Camera Roll\\\\IMG-20210330-WA0003.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e691ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\image_captioning\\\\cocoapi\\\\images\\\\test2014\\\\COCO_test2014_000000000750.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\image_captioning\\\\cocoapi\\\\images\\\\test2014\\\\COCO_test2014_000000001371.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"C:\\\\image_captioning\\\\cocoapi\\\\images\\\\test2014\\\\COCO_test2014_000000002082.jpg\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
