{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch for any changes in vocabulary.py, data_loader.py, utils.py or model.py, and re-load it automatically.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from pycocotools.coco import COCO\n",
    "import math\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from utils import train, validate, save_epoch, early_stopping\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Set values for the training variables\n",
    "batch_size = 32        \n",
    "vocab_threshold = 5     \n",
    "vocab_from_file = True  \n",
    "embed_size = 256        \n",
    "hidden_size = 512       \n",
    "num_epochs = 4          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a transform to pre-process the training images\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          \n",
    "    transforms.RandomCrop(224),                      \n",
    "    transforms.RandomHorizontalFlip(),               \n",
    "    transforms.ToTensor(),                           \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),     \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Define a transform to pre-process the validation images\n",
    "transform_val = transforms.Compose([ \n",
    "    transforms.Resize(256),                          \n",
    "    transforms.CenterCrop(224),                      \n",
    "    transforms.ToTensor(),                           \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.88s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 414113/414113 [00:41<00:00, 9871.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.46s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 202654/202654 [00:19<00:00, 10362.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build data loader, applying the transforms\n",
    "train_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "val_loader = get_loader(transform=transform_val,\n",
    "                         mode='val',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "\n",
    "# The size of the vocabulary\n",
    "vocab_size = len(train_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(params=params, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training steps: 12942\n",
      "Number of validation steps: 6333\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of training and validation steps per epoch\n",
    "total_train_step = math.ceil(len(train_loader.dataset.caption_lengths) / train_loader.batch_sampler.batch_size)\n",
    "total_val_step = math.ceil(len(val_loader.dataset.caption_lengths) / val_loader.batch_sampler.batch_size)\n",
    "print (\"Number of training steps:\", total_train_step)\n",
    "print (\"Number of validation steps:\", total_val_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of train and validation losses and validation Bleu-4 scores by epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_bleus = []\n",
    "# Keep track of the current best validation Bleu score\n",
    "best_val_bleu = float(\"-INF\")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(4, num_epochs ):\n",
    "    train_loss = train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "                       vocab_size, epoch, total_train_step)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_bleu = validate(val_loader, encoder, decoder, criterion,\n",
    "                                  train_loader.dataset.vocab, epoch, total_val_step)\n",
    "    val_losses.append(val_loss)\n",
    "    val_bleus.append(val_bleu)\n",
    "    if val_bleu > best_val_bleu:\n",
    "        print (\"Validation Bleu-4 improved from {:0.4f} to {:0.4f}, saving model to best-model.pkl\".\n",
    "               format(best_val_bleu, val_bleu))\n",
    "        best_val_bleu = val_bleu\n",
    "        filename = os.path.join(\"./models\", \"best-model.pkl\")\n",
    "        save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "                   val_bleu, val_bleus, epoch)\n",
    "    else:\n",
    "        print (\"Validation Bleu-4 did not improve, saving model to model-{}.pkl\".format(epoch))\n",
    "    # Save the entire model anyway, regardless of being the best model so far or not\n",
    "    filename = os.path.join(\"./models\", \"model-{}.pkl\".format(epoch))\n",
    "    save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "               val_bleu, val_bleus, epoch)\n",
    "    print (\"Epoch [%d/%d] took %ds\" % (epoch, num_epochs, time.time() - start_time))\n",
    "    if epoch > 5:\n",
    "        # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "        if early_stopping(val_bleus, 3):\n",
    "            break\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train step [12942/12942], 175s, Loss: 2.0197, Perplexity: 7.53603"
     ]
    }
   ],
   "source": [
    "# Load the last checkpoints\n",
    "checkpoint = torch.load(os.path.join('./models', 'train-model-412900.pkl'))\n",
    "\n",
    "# Load the pre-trained weights\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Load start_loss from checkpoint if in the middle of training process; otherwise, comment it out\n",
    "start_loss = checkpoint['total_loss']\n",
    "\n",
    "# Load epoch. Add 1 if we start a new epoch\n",
    "epoch = checkpoint['epoch']\n",
    "# Load start_step from checkpoint if in the middle of training process; otherwise, comment it out\n",
    "start_step = checkpoint['train_step'] + 1\n",
    "\n",
    "# Train 1 epoch at a time due to very long training time\n",
    "train_loss = train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "                   vocab_size, epoch, total_train_step, start_step, start_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.356230806823566, 2.3558660150955935, 2.023856584595816] [2.2138556149565702, 2.2143045429669153] [0.11517005905200531, 0.11513478912242286] 0.11946923720486312\n",
      "Training completed for epoch 4, saving model to train-model-4.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoints\n",
    "train_checkpoint = torch.load(os.path.join('./models', 'train-model-412900.pkl'))\n",
    "epoch_checkpoint = torch.load(os.path.join('./models', 'model-3.pkl'))\n",
    "best_checkpoint = torch.load(os.path.join('./models', 'best-model.pkl'))\n",
    "\n",
    "# Load the pre-trained weights and epoch from the last train step\n",
    "encoder.load_state_dict(train_checkpoint['encoder'])\n",
    "decoder.load_state_dict(train_checkpoint['decoder'])\n",
    "optimizer.load_state_dict(train_checkpoint['optimizer'])\n",
    "epoch = train_checkpoint['epoch']\n",
    "\n",
    "# Load from the previous epoch\n",
    "train_losses = epoch_checkpoint['train_losses']\n",
    "val_losses = epoch_checkpoint['val_losses']\n",
    "val_bleus = epoch_checkpoint['val_bleus']\n",
    "\n",
    "# Load from the best model\n",
    "best_val_bleu = best_checkpoint['val_bleu']\n",
    "\n",
    "train_losses.append(train_loss)\n",
    "print (train_losses, val_losses, val_bleus, best_val_bleu)\n",
    "print (\"Training completed for epoch {}, saving model to train-model-{}.pkl\".format(epoch, epoch))\n",
    "filename = os.path.join(\"./models\", \"train-model-{}.pkl\".format(epoch))\n",
    "save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "           best_val_bleu, val_bleus, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Val step [5900/6333], 369s, Loss: 2.2063, Perplexity: 9.0817, Bleu-4: 0.13432\n",
      "Epoch 4, Val step [6000/6333], 378s, Loss: 2.9563, Perplexity: 19.2258, Bleu-4: 0.1051\n",
      "Epoch 4, Val step [6100/6333], 368s, Loss: 1.8908, Perplexity: 6.6245, Bleu-4: 0.13504\n",
      "Epoch 4, Val step [6200/6333], 369s, Loss: 2.0759, Perplexity: 7.9716, Bleu-4: 0.13631\n",
      "Epoch 4, Val step [6300/6333], 369s, Loss: 2.4022, Perplexity: 11.0471, Bleu-4: 0.0686\n",
      "Epoch 4, Val step [6333/6333], 126s, Loss: 2.4394, Perplexity: 11.4666, Bleu-4: 0.0940"
     ]
    }
   ],
   "source": [
    "# Load the last checkpoint\n",
    "checkpoint = torch.load(os.path.join('./models', 'val-model-45800.pkl'))\n",
    "\n",
    "# Load the pre-trained weights\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "# Load these from checkpoint if in the middle of validation process; otherwise, comment them out\n",
    "start_loss = checkpoint['total_loss']\n",
    "start_bleu = checkpoint['total_bleu_4']\n",
    "\n",
    "# Load epoch\n",
    "epoch = checkpoint['epoch']\n",
    "# Load start_step from checkpoint if in the middle of training process; otherwise, comment it out\n",
    "start_step = checkpoint['val_step'] + 1\n",
    "\n",
    "# Validate 1 epoch at a time due to very long validation time\n",
    "val_loss, val_bleu = validate(val_loader, encoder, decoder, criterion, \n",
    "                              train_loader.dataset.vocab, epoch, total_val_step, \n",
    "                              start_step, start_loss, start_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.356230806823566, 2.3558660150955935, 2.023856584595816] [2.2138556149565702, 2.2143045429669153, 2.171720981146136] [0.11517005905200531, 0.11513478912242286, 0.12158010500298949] 0.11946923720486312\n",
      "Validation Bleu-4 improved from 0.1195 to 0.1216, saving model to best-model.pkl\n",
      "0.12158010500298949\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoints`\n",
    "checkpoint = torch.load(os.path.join('./models', 'train-model-4.pkl'))\n",
    "best_checkpoint = torch.load(os.path.join('./models', 'best-model.pkl'))\n",
    "\n",
    "# Load the pre-trained weights\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Load train and validation losses and validation Bleu-4 scores \n",
    "train_losses = checkpoint['train_losses']\n",
    "val_losses = checkpoint['val_losses']\n",
    "val_bleus = checkpoint['val_bleus']\n",
    "best_val_bleu = best_checkpoint['val_bleu']\n",
    "\n",
    "# Load epoch\n",
    "epoch = checkpoint['epoch']    \n",
    "\n",
    "val_losses.append(val_loss)\n",
    "val_bleus.append(val_bleu)\n",
    "print (train_losses, val_losses, val_bleus, best_val_bleu)\n",
    "\n",
    "if val_bleu > best_val_bleu:\n",
    "    print (\"Validation Bleu-4 improved from {:0.4f} to {:0.4f}, saving model to best-model.pkl\".\n",
    "           format(best_val_bleu, val_bleu))\n",
    "    best_val_bleu = val_bleu\n",
    "    print (best_val_bleu)\n",
    "    filename = os.path.join(\"./models\", \"best-model.pkl\")\n",
    "    save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "               val_bleu, val_bleus, epoch)\n",
    "else:\n",
    "    print (\"Validation Bleu-4 did not improve, saving model to model-{}.pkl\".format(epoch))\n",
    "# Save the entire model anyway, regardless of being the best model so far or not\n",
    "filename = os.path.join(\"./models\", \"model-{}.pkl\".format(epoch))\n",
    "save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "           val_bleu, val_bleus, epoch)\n",
    "if epoch > 5:\n",
    "    # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "    if early_stopping(val_bleus, 3):\n",
    "        print (\"Val Bleu-4 doesn't improve anymore. Early stopping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
